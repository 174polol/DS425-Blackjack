{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was written using Python 3.11.3 as kernel. \n",
    "Installation of the Open AI Gym package is necessary (run \"pip install gym; pip install gym[atari]\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Blackjack-v1')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next two cells can be used to manually play the game. env.reset() resets the game and gives a tuple containing the player hand, the dealers card up, and if the player has a usable ace. env.step Lets you take an action. Using it with argument '1' makes you hit a new card, argument '0' makes you stick. It's output is the same tuple as env.reset(), followed by the reward for last action, i.e. -1 for an action that made you lose (busting or sticking but having less than the dealer), 0 if you tie after sticking or not busting after hitting, and +1 for winning (sticking without busting but still having more than the dealer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10, 8, False), {})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((18, 8, False), 0.0, False, False, {})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here on, the Reinforcement Learning (RL) agent is implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the state space\n",
    "player_sum_space = range(4, 22)  # possible player hand values (4, 5, ..., 20, 21)\n",
    "dealer_card_space = range(1, 11)  # possible dealer up card values (2, 3, ..., 10, 11)\n",
    "usable_ace_space = [False, True]  # whether the player has a usable ace\n",
    "\n",
    "state_space = []\n",
    "for player_sum in player_sum_space:\n",
    "    for dealer_card in dealer_card_space:\n",
    "        for usable_ace in usable_ace_space:\n",
    "            state_space.append((player_sum, dealer_card, usable_ace))\n",
    "\n",
    "# This creates an array of length 306, which is the number of possible states in the game of blackjack. \n",
    "# Each state is a tuple of the player’s sum, the dealer’s card, and whether the player has a usable ace.\n",
    "\n",
    "# Define the action space\n",
    "action_space = [0, 1]  # hit or stick\n",
    "\n",
    "# Get the size of the state and action spaces\n",
    "num_states = len(state_space)\n",
    "num_actions = len(action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:01<00:00, 7292.19it/s]\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Q-table with initial values. \n",
    "Q_table_init = np.zeros((num_states, num_actions))\n",
    "\n",
    "for i in trange(10000):\n",
    "    # Initialize the state\n",
    "    state = env.reset()[0]\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        # Choose an action\n",
    "        action = np.random.choice(action_space)\n",
    "\n",
    "        # Take the action\n",
    "        next_state, reward, done, terminal, dic = env.step(action)\n",
    "\n",
    "        # Update Q-table\n",
    "        Q_table_init[state_space.index(state)][action] += reward\n",
    "\n",
    "        # Update state\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:13<00:00, 7175.34it/s]\n"
     ]
    }
   ],
   "source": [
    "### Retrain the agent. Takes about 15 seconds to run on my machine. ###\n",
    "# Set Q_table\n",
    "Q_table = Q_table_init\n",
    "\n",
    "# Hyperparameters\n",
    "num_episodes = 100000  # Total number of episodes\n",
    "alpha = 0.075  # Learning rate\n",
    "gamma = 0.90  # Discount factor\n",
    "epsilon = 0.1  # Epsilon-greedy parameter\n",
    "\n",
    "# Training loop\n",
    "for episode in trange(num_episodes):\n",
    "    state = env.reset()[0]\n",
    "    done = False\n",
    "    while not done:\n",
    "        # Epsilon-greedy action selection\n",
    "        if np.random.random() < epsilon:\n",
    "            action = np.random.choice(action_space)\n",
    "        else:\n",
    "            action = np.argmax(state_space.index(state))\n",
    "\n",
    "        # Take the action\n",
    "        next_state, reward, done, terminal, dic = env.step(action)\n",
    "\n",
    "        # Update Q-value if you busted, necessary because Q_table(next_state) does not exist if you bust\n",
    "        if next_state not in state_space:\n",
    "            Q_table[state_space.index(state)][action] += alpha * (reward - Q_table[state_space.index(state)][action])\n",
    "            break\n",
    "\n",
    "        # Update Q-value for current state-action pair\n",
    "        Q_table[state_space.index(state)][action] += alpha * (reward + gamma * np.max(Q_table[state_space.index(next_state)]) - Q_table[state_space.index(state)][action])\n",
    "\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000000/1000000 [02:06<00:00, 7890.38it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4309124974614634"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Make the agent play many games and check winnning rate. Takes about 2 minutes and 5 seconds to run on my machine ###\n",
    "\n",
    "num_games = 1000000\n",
    "num_wins = 0\n",
    "num_draws = 0\n",
    "num_losses = 0\n",
    "\n",
    "for i in trange(num_games):\n",
    "    state = env.reset()[0]\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = np.argmax(Q_table[state_space.index(state)])\n",
    "        next_state, reward, done, terminal, dic = env.step(action)\n",
    "        state = next_state\n",
    "        if done and reward == 1:\n",
    "            num_wins += 1\n",
    "        elif done and reward == 0:\n",
    "            num_draws += 1\n",
    "        elif done and reward == -1:\n",
    "            num_losses += 1\n",
    "\n",
    "num_wins/(num_games-num_draws) # Winrate excluding draws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000000/1000000 [02:05<00:00, 7940.03it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.29292192864980326"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check winnning rate of random action policy\n",
    "# This has been run on a large amount of games so it is a good estimate of the win rate. Don't run it again, it takes a long time.\n",
    "\n",
    "num_games = 1000000\n",
    "num_wins = 0\n",
    "num_draws = 0\n",
    "num_losses = 0\n",
    "\n",
    "for i in trange(num_games):\n",
    "    state = env.reset()[0]\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = np.random.choice(action_space)\n",
    "        next_state, reward, done, terminal, dic = env.step(action)\n",
    "        state = next_state\n",
    "        if done and reward == 1:\n",
    "            num_wins += 1\n",
    "        elif done and reward == 0:\n",
    "            num_draws += 1\n",
    "        elif done and reward == -1:\n",
    "            num_losses += 1\n",
    "\n",
    "num_wins/(num_games-num_draws) # Winrate excluding draws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [1:19:20<00:00, 952.10s/it]\n"
     ]
    }
   ],
   "source": [
    "# Look for winrates with different hyperparameters, should run in about 60 minutes on my machine\n",
    "\n",
    "# Hyperparameter ranges\n",
    "num_episodes = 100000  # Total number of episodes\n",
    "num_games = 100000 # Number of games to play to check winrate\n",
    "alphas = np.arange(0.02,0.12,0.02)  # Learning rate\n",
    "gammas = np.arange(0.89,1.01,0.02)  # Discount factor\n",
    "epsilons = np.arange(0.05,0.35,0.05)  # Epsilon-greedy parameter\n",
    "\n",
    "winrates=np.zeros((len(alphas),len(gammas),len(epsilons)))\n",
    "\n",
    "# Training loop\n",
    "for ii in trange(len(alphas)):\n",
    "    alpha=alphas[ii]\n",
    "    for jj in range(len(gammas)):\n",
    "        gamma=gammas[jj]\n",
    "        for kk in range(len(epsilons)):\n",
    "            epsilon=epsilons[kk]\n",
    "            # Set Q_table\n",
    "            Q_table = Q_table_init\n",
    "\n",
    "            for episode in range(num_episodes):\n",
    "                state = env.reset()[0]\n",
    "                done = False\n",
    "                while not done:\n",
    "                    # Epsilon-greedy action selection\n",
    "                    if np.random.random() < epsilon:\n",
    "                        action = np.random.choice(action_space)\n",
    "                    else:\n",
    "                        action = np.argmax(state_space.index(state))\n",
    "\n",
    "                    # Take the action\n",
    "                    next_state, reward, done, terminal, dic = env.step(action)\n",
    "\n",
    "                    # Update Q-value if you busted, necessary because Q_table(next_state) does not exist if you bust\n",
    "                    if next_state not in state_space:\n",
    "                        Q_table[state_space.index(state)][action] += alpha * (reward - Q_table[state_space.index(state)][action])\n",
    "                        break\n",
    "\n",
    "                    # Update Q-value for current state-action pair\n",
    "                    Q_table[state_space.index(state)][action] += alpha * (reward + gamma * np.max(Q_table[state_space.index(next_state)]) - Q_table[state_space.index(state)][action])\n",
    "\n",
    "                    state = next_state\n",
    "            num_games = 100000\n",
    "            num_wins = 0\n",
    "            num_draws = 0\n",
    "            num_losses = 0\n",
    "\n",
    "            for i in range(num_games):\n",
    "                state = env.reset()[0]\n",
    "                done = False\n",
    "                while not done:\n",
    "                    action = np.argmax(Q_table[state_space.index(state)])\n",
    "                    next_state, reward, done, terminal, dic = env.step(action)\n",
    "                    state = next_state\n",
    "                    if done and reward == 1:\n",
    "                        num_wins += 1\n",
    "                    elif done and reward == 0:\n",
    "                        num_draws += 1\n",
    "                    elif done and reward == -1:\n",
    "                        num_losses += 1\n",
    "\n",
    "            winrates[ii,jj,kk]=num_wins/(num_games-num_draws) # Winrate excluding draws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.3791973 , 0.3800415 , 0.40833786, 0.40786464, 0.40666139,\n",
       "         0.42847636],\n",
       "        [0.41961583, 0.40904543, 0.42064424, 0.42810593, 0.42096744,\n",
       "         0.42712573],\n",
       "        [0.42123593, 0.41707953, 0.42605964, 0.40270625, 0.42272266,\n",
       "         0.42665969],\n",
       "        [0.42035105, 0.42533449, 0.42635075, 0.41500554, 0.42425466,\n",
       "         0.42863056],\n",
       "        [0.42488498, 0.40868698, 0.42779173, 0.40948341, 0.42038529,\n",
       "         0.43365791],\n",
       "        [0.41835036, 0.41183618, 0.42962012, 0.43863763, 0.43040101,\n",
       "         0.43872549]],\n",
       "\n",
       "       [[0.37888426, 0.38675466, 0.38728473, 0.41420336, 0.4292439 ,\n",
       "         0.4249016 ],\n",
       "        [0.42667788, 0.4244682 , 0.42601699, 0.43682477, 0.43508541,\n",
       "         0.42229424],\n",
       "        [0.42340555, 0.40809392, 0.40164484, 0.43399416, 0.41530001,\n",
       "         0.40576694],\n",
       "        [0.40627516, 0.42878945, 0.43295112, 0.41021724, 0.44644604,\n",
       "         0.40308846],\n",
       "        [0.4088886 , 0.43767108, 0.44060232, 0.40983695, 0.41708341,\n",
       "         0.43266454],\n",
       "        [0.43911992, 0.44843917, 0.44112196, 0.43424533, 0.43903712,\n",
       "         0.43667556]],\n",
       "\n",
       "       [[0.33439985, 0.40272871, 0.42902807, 0.4094923 , 0.42494825,\n",
       "         0.43224299],\n",
       "        [0.43309218, 0.43134888, 0.42935536, 0.44171146, 0.44890339,\n",
       "         0.42795084],\n",
       "        [0.40731683, 0.42452295, 0.42572873, 0.41784633, 0.43619596,\n",
       "         0.42031318],\n",
       "        [0.40823294, 0.43690753, 0.41551084, 0.44087799, 0.42580757,\n",
       "         0.44449902],\n",
       "        [0.43407677, 0.42626818, 0.43380987, 0.42769499, 0.42530638,\n",
       "         0.43781415],\n",
       "        [0.41697032, 0.43902652, 0.44319106, 0.44136591, 0.44168055,\n",
       "         0.44556926]],\n",
       "\n",
       "       [[0.35199164, 0.37723253, 0.41723651, 0.42663481, 0.42388769,\n",
       "         0.44036858],\n",
       "        [0.42268333, 0.4249801 , 0.41719655, 0.44111349, 0.43389047,\n",
       "         0.41764411],\n",
       "        [0.44043294, 0.41338292, 0.43737729, 0.4395723 , 0.43377793,\n",
       "         0.41719635],\n",
       "        [0.43001328, 0.43157058, 0.42378309, 0.41198396, 0.41606443,\n",
       "         0.43009597],\n",
       "        [0.42722729, 0.41715361, 0.42350605, 0.44481024, 0.44647047,\n",
       "         0.4344191 ],\n",
       "        [0.44438862, 0.4437825 , 0.43662844, 0.43069429, 0.43115837,\n",
       "         0.43318777]],\n",
       "\n",
       "       [[0.35740534, 0.38457226, 0.42911998, 0.44666295, 0.43213242,\n",
       "         0.43936572],\n",
       "        [0.43633462, 0.44198133, 0.4461332 , 0.42826571, 0.42716883,\n",
       "         0.43245832],\n",
       "        [0.41262215, 0.43291234, 0.43446411, 0.42035996, 0.4160098 ,\n",
       "         0.42883645],\n",
       "        [0.43536665, 0.42763044, 0.43187518, 0.43478403, 0.43963514,\n",
       "         0.42532083],\n",
       "        [0.42681848, 0.43623752, 0.45016559, 0.44263115, 0.44302193,\n",
       "         0.43838139],\n",
       "        [0.43219623, 0.44675477, 0.43529336, 0.43518478, 0.42809562,\n",
       "         0.42317855]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "winrates # Open in a text editor to see all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.02, 0.04, 0.06, 0.08])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alphas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
